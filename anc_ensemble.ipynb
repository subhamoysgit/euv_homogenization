{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"anc_ensemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORj8nAbFHheIWxFwfHhvs5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZOHSPlHcs0mJ"},"source":["# NN options\n","n_ensemble = 10\t# no. NNs in ensemble\n","reg = 'anc'\t\t# type of regularisation to use - anc (anchoring) reg (regularised) free (unconstrained)\n","n_hidden = 512 \t# no. hidden units in NN\n","#activation_in = 'relu' # tanh relu sigmoid\n","\n","# optimisation options\n","epochs = 5 \t\t# run reg for 15+ epochs seems to mess them up\n","l_rate = 0.005 \t\t# learning rate\n","\n","# data options\n","n_data = len(fileList_train) \t# no. training data points\n","n_classes = 1 \t# no. classes predicting\n","n_Xdim = 512 \t# no. features for X\n","seed_in = 0 # random seed used to produce data blobs - try changing to see how results look w different data\n","\n","# variance of priors\n","W1_var = 15/n_Xdim\t\t# 1st layer weights and biases\n","W_mid_var = 1/n_hidden\t# 2nd layer weights and biases\n","W_last_var = 5/n_hidden\t# 3rd layer weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68j2664BtALB"},"source":["from keras.backend import sigmoid\n","def swish(x, beta = 1):\n","    return (x * sigmoid(beta * x))\n","\n","from keras.utils.generic_utils import get_custom_objects\n","from keras.layers import Activation\n","get_custom_objects().update({'swish': Activation(swish)})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC2kQ9EmtNFk"},"source":["# NN loss\n","#def fn_my_loss(y_true,y_pred):\n","#\treturn K.binary_crossentropy(y_true, y_pred, from_logits=True)\n","\n","# NN object\n","def fn_make_NN(reg='anc'):\n","\t# get initialisations, and regularisation values\n","  W_c1_lambda = 1/(2*W1_var)\n","  W_c1_anc = np.random.normal(loc=0,scale=np.sqrt(W1_var),size=[3,3,64,128])\n","  W_c1_init = np.random.normal(loc=0,scale=np.sqrt(W1_var),size=[3,3,64,128])\n","\n","  b1_var = W1_var\n","  b_c1_lambda = 1/(2*b1_var)\n","  b_c1_anc = np.random.normal(loc=0,scale=np.sqrt(b1_var),size=[128])\n","  b_c1_init = np.random.normal(loc=0,scale=np.sqrt(b1_var),size=[128])\n","\n","  W1_lambda = 1/(2*W1_var)\n","  W1_anc = np.random.normal(loc=0,scale=np.sqrt(W1_var),size=[n_Xdim,n_hidden])\n","  W1_init = np.random.normal(loc=0,scale=np.sqrt(W1_var),size=[n_Xdim,n_hidden])\n","\n","  b1_var = W1_var\n","  b1_lambda =  1/(2*b1_var)\n","  b1_anc = np.random.normal(loc=0,scale=np.sqrt(b1_var),size=[n_hidden])\n","  b1_init = np.random.normal(loc=0,scale=np.sqrt(b1_var),size=[n_hidden])\n","\n","  W_mid_lambda = 1/(2*W_mid_var)\n","  W_mid_anc = np.random.normal(loc=0,scale=np.sqrt(W_mid_var),size=[n_hidden,n_hidden])\n","  W_mid_init = np.random.normal(loc=0,scale=np.sqrt(W_mid_var),size=[n_hidden,n_hidden])\n","\n","  b_mid_var = W_mid_var\n","  b_mid_lambda =  1/(2*b_mid_var)\n","  b_mid_anc = np.random.normal(loc=0,scale=np.sqrt(b_mid_var),size=[n_hidden])\n","  b_mid_init = np.random.normal(loc=0,scale=np.sqrt(b_mid_var),size=[n_hidden])\n","    \n","  W_last_lambda = 1/(2*W_last_var)\n","  W_last_anc = np.random.normal(loc=0,scale=np.sqrt(W_last_var),size=[n_hidden, n_classes])\n","  W_last_init = np.random.normal(loc=0,scale=np.sqrt(W_last_var),size=[n_hidden, n_classes])\n","\n","  # create custom regularised\n","  def custom_reg_W_c1(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * W_c1_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - W_c1_anc)) * W_c1_lambda/n_data\n","\n","  def custom_reg_b_c1(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * b_c1_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - b_c1_anc)) * b_c1_lambda/n_data\n","\n","\n","  def custom_reg_W1(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * W1_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - W1_anc)) * W1_lambda/n_data\n","\n","  def custom_reg_b1(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * b1_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - b1_anc)) * b1_lambda/n_data\n","\n","  def custom_reg_W_mid(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * W_mid_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - W_mid_anc)) * W_mid_lambda/n_data\n","\n","  def custom_reg_b_mid(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * b_mid_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - b_mid_anc)) * b_mid_lambda/n_data\n","\n","  def custom_reg_W_last(weight_matrix):\n","    if reg == 'reg':\n","      return K.sum(K.square(weight_matrix)) * W_last_lambda/n_data\n","    elif reg == 'free':\n","      return 0.\n","    elif reg == 'anc':\n","      return K.sum(K.square(weight_matrix - W_last_anc)) * W_last_lambda/n_data\n","\n","  model = Sequential()\n","  model.add(Conv2D(32, (3, 3), input_shape = (90,90,225),padding='same')) \n","  model.add(Activation('swish'))\n","  model.add(BatchNormalization()) \n","  model.add(Conv2D(32, (3, 3),padding='same')) \n","  model.add(Activation('swish')) \n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n","\n","  #(45,45,32)\n","  model.add(Conv2D(64, (3, 3))) \n","  model.add(Activation('swish'))\n","  model.add(BatchNormalization()) \n","  model.add(Conv2D(64, (3, 3))) \n","  model.add(Activation('swish')) \n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n","\n","  #(20,20,64)\n","  model.add(Conv2D(128, (3, 3),kernel_initializer=keras.initializers.Constant(value=W_c1_init),bias_initializer=keras.initializers.Constant(value=b_c1_init),kernel_regularizer=custom_reg_W_c1,bias_regularizer=custom_reg_b_c1)) \n","  model.add(Activation('swish'))\n","  model.add(BatchNormalization()) \n","  model.add(Conv2D(128, (3, 3))) \n","  model.add(Activation('swish')) \n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n","\n","\n","\n","  #(8,8,128)\n","  model.add(Conv2D(256, (3, 3))) \n","  model.add(Activation('swish'))\n","  model.add(BatchNormalization()) \n","  model.add(Conv2D(256, (3, 3))) \n","  model.add(Activation('swish')) \n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size =(2, 2), strides=(2,2))) \n","\n","\n","  #(2,2,256)\n","  model.add(Conv2D(512, (2, 2))) \n","  model.add(Activation('swish'))\n","\n","\n","  #(1,1,512)  \n","  model.add(Flatten()) \n","  model.add(Dense(512,input_shape=(n_Xdim,),kernel_initializer=keras.initializers.Constant(value=W1_init),bias_initializer=keras.initializers.Constant(value=b1_init),kernel_regularizer=custom_reg_W1,bias_regularizer=custom_reg_b1)) \n","  model.add(Activation('swish')) \n","  #model.add(Dropout(0.5))#,seed=seed_value)) \n","  model.add(Dense(512,kernel_initializer=keras.initializers.Constant(value=W_mid_init),bias_initializer=keras.initializers.Constant(value=b_mid_init),kernel_regularizer=custom_reg_W_mid,bias_regularizer=custom_reg_b_mid)) \n","  model.add(Activation('swish')) \n","  #model.add(Dropout(0.5))#,seed=seed_value))\n","  model.add(Dense(1,use_bias=False,kernel_initializer=keras.initializers.Constant(value=W_last_init),kernel_regularizer=custom_reg_W_last)) \n","  model.add(Activation('sigmoid'))\n","  sgd = SGD(lr=0.0001, momentum=0.9, nesterov=True)\n","  model.compile(loss='binary_crossentropy', optimizer=sgd,metrics=['accuracy'])\n","\n","  return model\n","\n","def fn_predict_ensemble(NNs, x_test):\n","\t''' fn to predict given a list of NNs (an ensemble)''' \n","\ty_prob_preds = []\n","\tfor m in range(len(NNs)):\n","\t\ty_prob_preds.append(NNs[m].predict(x_test, verbose=0))\n","\ty_prob_preds = np.array(y_prob_preds)\n","\n","\ty_prob_final = np.mean(y_prob_preds,axis=0)\n","\n","\treturn y_prob_preds, y_prob_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDm2BzOWtZy1"},"source":["# create some data\n","#x_train, y_train, x_test, y_test = fn_make_data(seed_in=seed_in)\n","\n","# create the NNs\n","NNs=[]\n","for m in range(n_ensemble):\n","  NNs.append(fn_make_NN(reg=reg))\n","print(NNs[-1].summary())\n","batch_size=10\n","# do the actual training\n","NNs_hist_train=[]; NNs_hist_val=[]\n","for m in range(n_ensemble):\n","  print('-- training: ' + str(m+1) + ' of ' + str(n_ensemble) + ' NNs --')\n","  hist=NNs[m].fit(imageLoaderNew(fileList_train, batch_size), steps_per_epoch = 4*len(fileList_train)/batch_size, epochs = 5, verbose=0,validation_data=imageLoaderNew(fileList_valid, batch_size), validation_steps = 4*len(fileList_valid)/batch_size)    \n","  NNs[m].save_weights('/content/drive/My Drive/Colab Notebooks/swri_research/code/flux_emerge_ensemble_model_'+str(m+1)+'.h5')\n","  print('-- NN: ' + str(m+1) + ' weights saved --')\n","  NNs_hist_train.append(hist.history['loss'])\n","  NNs_hist_val.append(hist.history['val_loss'])"],"execution_count":null,"outputs":[]}]}